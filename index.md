---
layout: default
title: Abhishek Jha
---



## Bio


I recently completed my PhD as an ELLIS doctoral student at KU Leuven, Belgium, where I was advised by [Prof. Tinne Tuytelaars](https://homes.esat.kuleuven.be/~tuytelaa) at the PSI division, Department of Electrical Engineering (ESAT), and co-supervised by [Dr. Yuki Asano](https://yukimasano.github.io/). My doctoral research focused on representation learning, particularly in developing robust and interpretable representations.

<!-- I am an ELLIS doctoral student at KU Leuven, Belgium. I am advised by [Prof. Tinne Tuytelaars](https://homes.esat.kuleuven.be/~tuytelaa) at PSI division, Departement Elektrotechniek (ESAT), and co-supervised by [Dr. Yuki Asano](https://yukimasano.github.io/). My PhD research is focused on representation learning in the direction of robust and interpretable representations. -->

Prior to joining the doctoral school I have spent a brief time visiting [IISc Bangalore](http://iisc.ac.in) as a Project Assistant at [MALL Lab](malllabiisc.github.io/), working with [Dr. Partha Pratim Talukdar](talukdar.net), [Dr. Anirban Chokraborty](cds.iisc.ac.in/news/author/anirban/) and [Dr. Anand Mishra](https://researchweb.iiit.ac.in/~anand.mishra/) on a project on Weakly supervised video understanding using Knowledge Graphs (KG).


 I completed my Masters (MS) at [IIIT Hyderabad](https://iiit.ac.in/), where I was jointly advised by  [Prof. C. V. Jawahar](http://faculty.iiit.ac.in/~jawahar/) and [Prof. Vinay P. Namboodiri](https://www.cse.iitk.ac.in/users/vinaypn/) at [Center for Visual Information Technology](http://cvit.iiit.ac.in/). My masters research was focused on solving Visual Speech Recognition (VSR) which lies at the intersection of multiple modalities like videos (speech videos) audios (speech audio) and texts (Natural language).  I have also worked in the space of Image stylization for enabling cross-modal transfer of style. 

<!--I am a Computer Science MS student at IIIT Hyderabad. I am advised by [Prof. C. V. Jawahar](http://faculty.iiit.ac.in/~jawahar/) and co-advised by [Prof. Vinay P. Namboodiri](https://www.cse.iitk.ac.in/users/vinaypn/) at [Center for Visual Information Technology](http://cvit.iiit.ac.in/). 
-->

<!--My research focuses on computer vision and machine learning for solving Visual Speech Recognition (VSR) which lies at the intersection of multiple modalities like videos (speech videos) audios (speech audio) and texts (Natural language).  I have also worked in the space of Image stylization for enabling cross-modal transfer of style. My goal is to develope robust and scalable solutions for real world sensing problems using computer vision.
-->

<!-- Prior to this, I have spent one year (2015-16) as a research fellow at CVIT working on a problem on cross-modal multimedia retrieval, under the supervision of Prof. Jawahar. -->
<!--Before moving to Hyderabad, I was a Manager (Planning), at [Tata Steel Limited](http://tatasteel.com/) (2014-15) working towards automation and energy consumption optimization in processing plant.
-->
<!-- I graduated from [IIT Dhanbad](http://iitism.ac.in), India, in 2014 with a B.Tech in Electronics and Communication Engineering. During my undergraduate years I worked closely with Prof. Mrinal Sen and [Dr. Dilip Prasad](https://sites.google.com/site/dilipprasad/) on projects related to computer vision and robotics. -->

***

## Updates

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;
-----|-----
**\[Mar 2025\]** | Successfully defended my PhD thesis at KU Leuven, titled: [Understanding and Transferring the Learning Dynamics of Visual Representations in the Absence of Labels](https://www.kuleuven.be/doctoraatsverdediging/fiches/3E20/3E200810.htm).
**\[Sep 2024\]** | **Accepted:** "Analysis of Spatial augmentation in Self-supervised models in the purview of training and test distributions" accepted at **ECCV 2024 Workshop on Out-of-distribution generalization in computer vision (OOD-CV)**.
**\[Feb 2024\]** | **Preprint:** [The Common Stability Mechanism behind most Self-Supervised Learning Approaches](https://arxiv.org/abs/2402.14957)
**\[Oct 2023\]** | I will be doing an ELLIS research visit at University of Amsterdam, Netherlands, under the supervision of Dr. [Yuki Asano](https://yukimasano.github.io/).
**\[Jul 2023\]** | **Attending:** [ICVSS 2023](https://iplab.dmi.unict.it/icvss2023/), Sicily, Italy. I will also be presenting a poster on Exploring the stability of Self-Supervised Representations.
**\[Oct 2022\]** | **Accepted:** Our paper "SimGlim: Simplifying glimpse based active visual reconstruction" in WACV 2023.
**\[Sep 2022\]** | **Accepted:** Our paper [Barlow constrained optimization for Visual Question Answering](https://arxiv.org/abs/2203.03727), in WACV 2023.
**\[Aug 2022\]** | I will be attending [ELLIS Doctoral symposium 2022](https://ellisalicante.org/eds2022/) in Alicante, Spain.
**\[Aug 2021\]** | **Accepted:** Our paper "Glimpse-Attend-and-Explore: Self-Attention for Active Visual Exploration" in ICCV 2021.
**\[Jan 2021\]** | Presented a poster on " Transferability of Self-Supervised Representations" in [Mediterranean Machine Learning  summer school 2021](https://www.m2lschool.org/).

<!-- 
**\[Sep 2020\]** | Will be attending [Mediterranean Machine Learning  summer school 2021](https://www.m2lschool.org/) in January 2021.
**\[Aug 2020\]** | Will be attending [AI summer school 2020 (online), AI Singapore](https://aisummerschool2020.aisingapore.org/).
**\[Nov 2019\]** | Joining KU Leuven, as a PhD student, at PSI ESAT.
**\[Jul 2019\]** | **Accepted:** Our paper "Towards Automatic Face-to-Face Translation" accepted in [ACM Multimedia 2019](https://www.acmmm.org/2019/).
**\[May 2019\]** | Vikram presented our work ["Cross-Language Speech Dependent Lip-Synchronization"](https://drive.google.com/file/d/1a88OiddZDcfrE1yh8UORnMXLWe_YOcHY/view?usp=sharing) in [ICASSP 2019](https://2019.ieeeicassp.org/), Brighton, UK.
**\[Apr 2019\]** | Presented my work on "Audio-Visual Speech Recognition and Synthesis" at [MPI-Informatics](https://www.mpi-inf.mpg.de/home/), Saarbrucken.
**\[Apr 2019\]** | Successfully defended my MS thesis **Audio-Visual Speech Recognition and Synthesis**. [Thesis Link](http://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.ab492d305f4ce6f0.416268697368656b5f6a68615f4d535f5468657369735f66696e616c5f64726166742832292e706466.pdf). 
**\[Feb 2019\]** | **Accepted:** "Cross-Language Speech Dependent Lip-Synchronization" accepted in ICASSP 2019.
**\[Feb 2019\]** | Will be spending next couple of months in **IISc Bangalore** as a visiting student.
**\[Jan 2019\]** | Submitted my MS thesis at IIIT Hyderabad.
**\[Dec 2018\]** | Paper "Spotting Words in Real World Videos : A Retrieval based approach" accepted in Journal of Machine Vision Application (MVA), Springer.
**\[Jul 2018\]** | Presenting our work on "Lip-Synchronization for Dubbed Instructional Videos" at 2nd Research Symposium, IIIT Hyderabad.
**\[May 2018\]** | Short paper "Lip-Synchronization for Dubbed Instructional Videos" accepted at [CVPR 2018 Workshop (FIVER)](http://fiver.eecs.umich.edu/).
**\[May 2018\]** | Giving a talk on "[Introduction to Image Style Transfer](/data/intro_to_photorealistic_Image_stylization.pdf)", at [CVIT](http://cvit.iiit.ac.in), IIIT Hyderabad.
**\[May 2018\]** | Paper "Cross-Modal Style Transfer" accepted at [ICIP](https://2018.ieeeicip.org/).
**\[Apr 2018\]** | Presenting our work on "Word-spotting in Silent Lip videos", at 1st Research Symposium, IIIT Hyderabad.
**\[Mar 2018\]** | Presenting our paper "Word-spotting in Silent Lip videos", at [WACV 2018](http://wacv18.wacv.net/), Lake Tahoe, CA.
**\[Fab 2018\]** | Organizing annual [R&D Showcase 2018](https://iiit.ac.in/randd/), at IIIT Hyderabad.
**\[Jan 2018\]** | Will be working as a "Mentor" [for Foundations of Artificial Intelligence and Machine Learning](https://www.talentsprint.com/aiml.dpl). -->





***

## Publications

Full list of publications can be found on [Google Scholar](https://scholar.google.com/citations?user=rdlsfogAAAAJ&hl=en).

&nbsp;



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" style="border-style: none ">


<tbody>
	<tr>
		<td width="35%"><img src="/images/analysis_spatial_augmentation_ssl.png" alt="Spatial_augment_SSL" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="#">Analysis of Spatial augmentation in Self-supervised models in the purview of training and test distributions</a> <br>
				<strong>Abhishek Jha</strong>, <a href="https://homes.esat.kuleuven.be/~tuytelaa/">Tinne Tuytelaars</a><br>
			<span style="color:#9A2617;">ECCV Workshop 2024, OOD-CV</span>
				<br>
				<a href="https://arxiv.org/abs/2409.18228">[Arxiv]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
In this paper, we present an empirical study of typical spatial augmentation techniques used in self-supervised representation learning methods (both contrastive and non-contrastive), namely random crop and cutout. Our contributions are: (a) we dissociate random cropping into two separate augmentations, overlap and patch, and provide a detailed analysis on the effect of area of overlap and patch size to the accuracy on down stream tasks. (b) We offer an insight into why cutout augmentation does not learn good representation, as reported in earlier literature. Finally, based on these analysis, (c) we propose a distance-based margin to the invariance loss for learning scene-centric representations for the downstream task on object-centric distribution, showing that as simple as a margin proportional to the pixel distance between the two spatial views in the scence-centric images can improve the learned representation. Our study furthers the understanding of the spatial augmentations, and the effect of the domain-gap between the training augmentations and the test distribution.</div>
			</p>
		</td>
		
	</tr>		
</tbody>










<tbody>
	<tr>
		<td width="35%"><img src="/images/barlow_vqa_website_image.jpg" alt="Barlow_VQA" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="#">Barlow constrained Visual Question Answering</a> <br>
				<strong>Abhishek Jha</strong>, <a href="https://badripatro.github.io/">Badri Patro</a>, <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html"> Luc Van Gool</a>, <a href="https://homes.esat.kuleuven.be/~tuytelaa/">Tinne Tuytelaars</a><br>
			<span style="color:#9A2617;">Winter conference on Computer vision (WACV) 2023</span>
				<br>
				<a href="https://arxiv.org/abs/2203.03727">[Arxiv]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
Visual question answering is a vision-and-language multimodal task, that aims at predicting answers given samples from the question and image modalities. Most recent methods focus on learning a good joint embedding space of images and questions, either by improving the interaction between these two modalities, or by making it a more discriminant space. However, how informative this joint space is, has not been well explored. In this paper, we propose a novel regularization for VQA models, Constrained Optimization using Barlow's theory (COB), that improves the information content of the joint space by minimizing the redundancy. It reduces the correlation between the learned feature components and thereby disentangles semantic concepts. Our model also aligns the joint space with the answer embedding space, where we consider the answer and image+question as two different `views' of what in essence is the same semantic information. We propose a constrained optimization policy to balance the categorical and redundancy minimization forces. When built on the state-of-the-art GGE model, the resulting model improves VQA accuracy by 1.4% and 4% on the VQA-CP v2 and VQA v2 datasets respectively. The model also exhibits better interpretability.
</div>
			</p>
		</td>
		
	</tr>		
</tbody>


















<tbody>
	<tr>
		<td width="35%"><img src="/images/simglim_website_image.jpg" alt="SimGlim_simplified_glimpse_active_visual_exploration" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="#">SimGlim: Simplified glimpse based active visual exploration</a> <br>
				<strong>Abhishek Jha</strong>, Soroush Seifi, <a href="https://homes.esat.kuleuven.be/~tuytelaa/">Tinne Tuytelaars</a> <br>
			<span style="color:#9A2617;">Winter conference on Computer vision (WACV) 2023</span>
				<!-- <br> -->
				<!-- <a href="https://arxiv.org/abs/2108.11717">[Arxiv]</a> -->
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
An agent with a limited field of view needs to sample the most informative local observations of an environment in order to model the global context. Current works train this selection strategy by defining a complex architecture built upon features learned through convolutional encoders. In this paper, we first discuss why vision transformers are better suited than CNNs for such an agent. Next, we propose a simple transformer based active visual sampling model, called ''SimGlim'', which utilises transformer's inherent self-attention architecture to sequentially predict the best next location based on the current observable environment. We show the efficacy of our proposed method on the task of image reconstruction in the partial observable setting and compare our model against existing state-of-the-art active visual reconstruction methods. Finally, we provide ablations for the parameters of our design choice to understand their importance in the overall architecture.

</div>
			</p>
		</td>
		
	</tr>		
</tbody>










<tbody>
	<tr>
		<td width="35%"><img src="/images/glatex_website_image.jpg" alt="Glimpse_Attend_and_Explore" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="#">Glimpse attend and explore: Self-Attention for Active Visual Exploration</a> <br>
				Soroush Seifi, <strong>Abhishek Jha</strong>, <a href="https://homes.esat.kuleuven.be/~tuytelaa/">Tinne Tuytelaars</a> <br>
			<span style="color:#9A2617;">International Conference on Computer Vision (ICCV) 2021</span>
				<br>
				<a href="https://arxiv.org/abs/2108.11717">[Arxiv]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
Active visual exploration aims to assist an agent with a limited field of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to address this problem either by using reinforcement learning, which is difficult to train, or by uncertainty maps, which are task-specific and can only be implemented for dense prediction tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-specific uncertainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to further improve the representations learned. Unlike previous works, we show the application of our model on multiple tasks like reconstruction, segmentation and classification. Our model provides encouraging results while being less dependent on dataset bias in driving the exploration. We further perform an ablation study to investigate the features and attention learned by our model. Finally, we show that our self-attention module learns to attend different regions of the scene by minimizing the loss on the downstream task.

</div>
			</p>
		</td>
		
	</tr>		
</tbody>
















<tbody>
	<tr>
		<td width="35%"><img src="/images/pic_icasssp_2019.png" alt="cross_lip_sync" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://drive.google.com/file/d/1a88OiddZDcfrE1yh8UORnMXLWe_YOcHY/view?usp=sharing">Cross-Language Speech Dependent Lip-Synchronization</a> <br>
				<strong>Abhishek Jha</strong>,
                <a href="https://voletiv.github.io/">Vikram Voleti</a>,
				<a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2019</span>
				<br>
				<a href="https://drive.google.com/file/d/1a88OiddZDcfrE1yh8UORnMXLWe_YOcHY/view?usp=sharing">[Link]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
Understanding videos of people speaking across international borders is hard as audiences from different demographies do not understand the language. Such speech videos are often supplemented with language subtitles. However, these hamper the viewing experience as the attention is shared. Simple audio dubbing in a different language makes the video appear unnatural due to unsynchronized lip motion. In this paper, we propose a system for automated cross-language lip synchronization for re-dubbed videos. Our model generates superior photorealistic lip-synchronization over original video in comparison to the current re-dubbing method. With the help of a user-based study, we verify that our method is preferred over unsynchronized videos.

</div>
			</p>
		</td>
		
	</tr>		
</tbody>








<tbody>
        <tr>
                <td width="35%"><img src="/images/mva_2018.png" alt="lip_word_spot" width="250" style="border-style: none"></td>
                <td width="65%" valign="top">
                        <p>
                                <a href="https://drive.google.com/file/d/1PXF46jspuTgMcXnyWcTgA90ABFI0LfyU/view?usp=sharing">
				Spotting Words in Silent Speech Videos : A Retrieval based approach</a>
                                <strong>Abhishek Jha</strong>,
                                <a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
                                <a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
                        <span style="color:#9A2617;">Journal of Machine Vision and Applications <strong>(MVA)</strong>, Springer, 2018</span>
                                <br><br>
                                <a href="https://drive.google.com/file/d/1PXF46jspuTgMcXnyWcTgA90ABFI0LfyU/view?usp=sharing">[Paper]</a>
                        </p>
                </td>

        </tr>
</tbody>


<tbody>
	<tr>
		<td width="35%"><img src="/images/cvpr_w_2018.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_A_Jha.pdf">Lip-Synchronization for Dubbed Instructional Videos</a> <br>
				<strong>Abhishek Jha</strong>,
				Vikram Voleti,
				<a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">FIVER, <strong>CVPR Workshop</strong> 2018</span>
				<br><br>
				<a href="http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_A_Jha.pdf">[Short Paper]</a> <a href="https://drive.google.com/file/d/19eTyXoDtKo_txxRRylg0mM9oQS7iKVaQ/view?usp=sharing">[Poster]</a>
			</p>
		</td>
		
	</tr>		
</tbody>



<tbody>
	<tr>
		<td width="35%"><img src="/images/icip_2018_3.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://drive.google.com/file/d/1lT8HNGrsUio9MW87XNattaUz5hsBkXYO/view?usp=sharing">Cross-modal style transfer</a> <br>
				Sahil Chelaramani,
				<strong>Abhishek Jha</strong>,
				<a href="https://faculty.iiit.ac.in/~anoop/">Anoop Namboodiri</a><br>
			<span style="color:#9A2617;">IEEE International Conference on Image Processing <strong>(ICIP)</strong> 2018</span>
				
				<br>
				<a href="https://drive.google.com/file/d/1lT8HNGrsUio9MW87XNattaUz5hsBkXYO/view?usp=sharing">[Paper]</a>
				<br><br>
				<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong>Abstract</strong><br>
				We, humans, have the ability to easily imagine scenes that
depict sentences such as “Today is a beautiful sunny day” or
“There is a Christmas feel, in the air”. While it is hard to

precisely describe what one person may imagine, the essen-
tial high-level themes associated with such sentences largely

remains the same. The ability to synthesize novel images that

depict the feel of a sentence is very useful in a variety of appli-
cations such as education, advertisement, and entertainment.

While existing papers tackle this problem given a style im-
age, we aim to provide a far more intuitive and easy to use

solution that synthesizes novel renditions of an existing im-
age, conditioned on a given sentence. We present a method

for cross-modal style transfer between an English sentence

and an image, to produce a new image that imbibes the essen-
tial theme of the sentence. We do this by modifying the style

transfer mechanism used in image style transfer to incorpo-
rate a style component derived from the given sentence. We

demonstrate promising results using the YFCC100m dataset.
</div>
			</p>
		</td>
		
	</tr>		
</tbody>







<tbody>
	<tr>
		<td width="35%"><img src="/images/wacv_2018.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://cvit.iiit.ac.in/images/ConferencePapers/2018/Word-Spotting-in-Silent-Lip-Videos.pdf">Word Spotting in Silent Lip Videos</a> <br>
				<strong>Abhishek Jha</strong>,
				<a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">IEEE  Winter Conference on Applications of Computer Vision <strong>(WACV)</strong> 2018</span>
				<br>
				<a href="https://cvit.iiit.ac.in/images/ConferencePapers/2018/Word-Spotting-in-Silent-Lip-Videos.pdf">[Paper]</a> <a href="https://drive.google.com/file/d/1XAE6gRhy2terH2DOmg87uEzpXzSDZgTk/view?usp=sharing">[Poster]</a> <a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/lip-word-spotting">[Project Page]</a>
				<br><br>

<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong>Abstract</strong><br>
				Our goal is to spot words in silent speech videos without
explicitly recognizing the spoken words, where the lip mo-
tion of the speaker is clearly visible and audio is absent. Ex-
isting work in this domain has mainly focused on recogniz-
ing a fixed set of words in word-segmented lip videos, which
limits the applicability of the learned model due to limited
vocabulary and high dependency on the model’s recogni-
tion performance.
Our contribution is two-fold:  1) we develop a pipeline
for  recognition-free  retrieval,  and  show  its  performance
against recognition-based retrieval on a large-scale dataset
and another set of out-of-vocabulary words.  2) We intro-
duce  a  query  expansion  technique  using  pseudo-relevant
feedback and propose a novel re-ranking method based on
maximizing the correlation between spatio-temporal land-
marks of the query and the top retrieval candidates.  Our
word  spotting  method  achieves  35%  higher  mean  aver-
age precision over recognition-based method on large-scale
LRW dataset. Finally, we demonstrate the application of the
method by word spotting in a popular speech video (“
The
great dictator
” by Charlie Chaplin) where we show that the
word retrieval can be used to understand what was spoken
perhaps in the silent movies.
	</div>
			</p>
		</td>
		
	</tr>		
</tbody>


<tbody>
	<tr>
		<td width="35%"><img src="/images/IJMIR_2018.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://link.springer.com/article/10.1007/s13735-017-0138-7">Cross-specificity:  modelling data semantics for cross-modal
matching and retrieval</a> <br>
				<a href="https://sites.google.com/view/yashaswiverma/">Yashaswi Verma</a>,
				<strong>Abhishek Jha</strong>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">International Journal of Multimedia Information Retrieval, Springer, June 2018</span>
				<br>
				<a href="https://link.springer.com/article/10.1007/s13735-017-0138-7">[Link]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
While dealing with multi-modal data such as pairs of images and text, though individual samples may demonstrate inherent heterogeneity in their content, they are usually coupled with each other based on some higher-level concepts such as their categories. This shared information can be useful in measuring semantics of samples across modalities in a relative manner. In this paper, we investigate the problem of analysing the degree of specificity in the semantic content of a sample in one modality with respect to semantically similar samples in another modality. Samples that have high similarity with semantically similar samples from another modality are considered to be specific, while others are considered to be relatively ambiguous. To model this property, we propose a novel notion of “cross-specificity”. We present two mechanisms to measure cross-specificity: one based on human judgement and other based on an automated approach. We analyse different aspects of cross-specificity and demonstrate its utility in cross-modal retrieval task. Experiments show that though conceptually simple, it can benefit several existing cross-modal retrieval techniques and provide significant boost in their performance.

</div>
			</p>
		</td>
		
	</tr>		
</tbody>







</table>



***

## Teaching

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;
-----|-----
Spring 2023: | Teaching assistant (TA) in the course **Image Analysis and Understanding (B-KUL-H09J2A)**, KU Leuven. Course instructor: [Prof. Tinne Tuytelaars](https://homes.esat.kuleuven.be/~tuytelaa) and [Dr. Marc Proesmans](https://www.esat.kuleuven.be/psi/members/00003449)
Spring 2022: |  Teaching assistant (TA) in the course **Information System and Signal Processing (B-KUL-H09M0A)**, KU Leuven. Course instructor: [Prof. Tinne Tuytelaars](https://homes.esat.kuleuven.be/~tuytelaa)
Spring 2021: |  Teaching assistant (TA) in the course **Information System and Signal Processing (B-KUL-H09M0A)**, KU Leuven. Course instructor: [Prof. Tinne Tuytelaars](https://homes.esat.kuleuven.be/~tuytelaa)
Spring 2020: |  Teaching assistant (TA) in the course **Information System and Signal Processing (B-KUL-H09M0A)**, KU Leuven. Course instructor: [Prof. Tinne Tuytelaars](https://homes.esat.kuleuven.be/~tuytelaa)
Monsoon 2018: | Teaching assistant (TA) in the course **Topics in Machine Learning (CSE975)**, IIIT Hyderabad. Course instructor: [Prof. Naresh Manwani](https://sites.google.com/site/nareshmanwani/home)
Spring 2018: | Mentor in 1st foundations course on **Artificial Intelligence and Machine Learning**. Course instructor: [Prof. C. V. Jawahar](http://faculty.iiit.ac.in/~jawahar/)


***

<!-- ## Services
- *Reviewer*: CVPR 2022, CVPR 2021, ICCV 2021, WACV 2021, WACV 2023.

- *Reviewer*: 7th [National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics](http://ncvpripg.kletech.ac.in/) (**NCVPRIPG 2019**), December 22-24, 2019, Hublie.

- *Reviewer*: Second IAPR International Conference on [Computer Vision & Image Processing](https://www.iitr.ac.in/cvip2017/) (**CVIP-2017**, September 10-12, 2017), IIT Roorkee.

- *Organizing Team*:  17th [R&D showcase 2018](http://iiit.ac.in/randd/), IIIT Hyderabad: showcase of exhibits and demonstration research projects and represents of IIIT-H’s most recent developments in research and innovation in technology.

<iframe width="560" height="315" src="https://www.youtube.com/embed/kj_P-it-ATE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

\[[Telangana Today](https://telanganatoday.com/iiit-hyderabad-to-organise-rd-showcase-2018-from-feb-24)\] \[[APN News](https://www.apnnews.com/iiit-hyderabad-celebrates-17th-convocation/)\]

- *Organizing Team*: 1st [Computer Vision Summer School](http://cvit.iiit.ac.in/summerschoolseries/) 2016, IIIT Hyderabad. -->

<!--- - *Volunteer*: Technical Exhibition 2015, Jamshedpur, Tata Steel.

- *Web Developer & Proceedings*: IEEE International Conference on Microwave and Photonics (**ICMAP**) 2013, [IIT Dhanbad](https://www.iitism.ac.in/).
 -->


<!-- ## Other Activity

- **[2016 - Present]**: Admin,  CVIT Lab, HPC cluster of  (aka Cosmos).

- **[2017 - Present]**: Student Admin, IIIT Hyderbad HPC cluster (aka ADA). -->

<!--- - **[2012 - 2013]**: Secretary, IEEE-Student branch, IIT(ISM) Dhanbad.

- **[2010 - 2012]**: Teacher, [Kartavya](http://www.kartavya.org/) , IIT(ISM) Dhanbad, an NGO for providing free and high quality education to underprivileged children living in slums and villages in india.

 -->






